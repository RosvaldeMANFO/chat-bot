import sqlite3
import random
import os
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    TextDataset, 
    DataCollatorForLanguageModeling
)
import torch

print(torch.cuda.get_device_name(0))

# Chemins absolus
current_dir = os.path.dirname(__file__)
db_path = os.path.abspath(os.path.join(current_dir, "data/database.sqlite"))
output_path = os.path.abspath(os.path.join(current_dir, "train.txt"))
model_output_dir = os.path.abspath(os.path.join(current_dir, "model_finetuned"))

def generer_train_file():
    
    # Templates am√©lior√©s
    question_templates = [
    "Dans quel club jouait {player} en {year} ?",
    "O√π √©voluait {player} lors de la saison {year} ?",
    "Pour quelle √©quipe jouait {player} en {year} ?",
        "Quel √©tait le club de {player} en {year} ?"
    ]
    answer_templates = [
        "{player} jouait pour {club} durant la saison {season}.",
        "En {season}, {player} √©tait au club {club}.",
        "{player} √©voluait √† {club} pendant la saison {season}."
    ]

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    dataset = []

    # G√©n√®re beaucoup d'exemples vari√©s
    cursor.execute("""
        SELECT p.player_name, t.team_long_name, m.season
        FROM Player p
        JOIN Match m ON m.home_player_1 = p.player_api_id
        JOIN Team t ON m.home_team_api_id = t.team_api_id
        WHERE p.player_name IS NOT NULL AND t.team_long_name IS NOT NULL
        LIMIT 2000
    """)
    for nom, club, saison in cursor.fetchall():
        year = saison[:4]
        for _ in range(2):  # 2 formulations diff√©rentes par exemple
            question = random.choice(question_templates).format(player=nom, year=year)
            reponse = random.choice(answer_templates).format(player=nom, club=club, season=saison)
            dataset.append((f"Utilisateur: {question}", f"Bot: {reponse}"))

    # Exemples n√©gatifs
    negative_questions = [
        "Dans quel club jouait Lionel Messi en 2050 ?",
        "Qui a gagn√© la Coupe du Monde 2022 ?",
        "Quel est le meilleur joueur de tous les temps ?"
    ]
    for nq in negative_questions:
        dataset.append((f"Utilisateur: {nq}", "Bot: Je ne sais pas r√©pondre √† cette question."))

    random.shuffle(dataset)
    with open(output_path, "w", encoding="utf-8") as f:
        for q, r in dataset:
            f.write(f"\n{q} {r}")

    print(f"‚úÖ {len(dataset)} paires g√©n√©r√©es dans {output_path}")

# G√©n√©ration du fichier d'entra√Ænement
generer_train_file()

# Chargement du mod√®le
model_name = "bigscience/bloomz-560m"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # Correction du token de padding
model = AutoModelForCausalLM.from_pretrained(model_name)

# Dataset adapt√© au dialogue
def load_dataset(file_path, tokenizer, block_size=128):
    return TextDataset(
        tokenizer=tokenizer,
        file_path=file_path,
        block_size=block_size
    )

train_dataset = load_dataset(output_path, tokenizer)

# Configuration d'entra√Ænement
training_args = TrainingArguments(
    output_dir=model_output_dir,
    overwrite_output_dir=True,
    num_train_epochs=3,  # Augmentation des epochs
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    save_strategy="epoch",
    learning_rate=2e-5,
    logging_dir="./logs",
    fp16=torch.cuda.is_available()
)

# Data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, 
    mlm=False
)

# Lancement de l'entra√Ænement
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset
)

print("üöÄ D√©but de l'entra√Ænement...")
trainer.train()
print("‚úÖ Entra√Ænement termin√©!")

# Sauvegarde finale
trainer.save_model()
tokenizer.save_pretrained(model_output_dir)
